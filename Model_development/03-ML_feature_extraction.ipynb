{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpxILFYaRB6n"
      },
      "source": [
        "# ML approach ( DL overfit like crazy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4ebVCsIKHXt",
        "outputId": "6b50a474-605b-47f7-92d1-900c1efcdf05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount= True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkA4YLpDRqed"
      },
      "source": [
        "## 1- Shape checks of the cleaned dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0763431",
        "outputId": "39d4b1b4-b65d-4019-8ef9-b7e459359e86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÑ Random subject file: /content/gdrive/MyDrive/modma_project/modma_per_subject/train/subject_2010016.npz\n",
            "X: shape = (480, 128, 251, 1)\n",
            "y: shape = (480,)\n",
            "subject: shape = ()\n",
            "\n",
            "X dtype: float64  | min: -9.381604389357635 max: 21.95066005968129\n",
            "y unique values: [1]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os, random\n",
        "from glob import glob\n",
        "\n",
        "# Paths\n",
        "train_dir = \"/content/gdrive/MyDrive/modma_project/modma_per_subject/train\"\n",
        "test_dir = \"/content/gdrive/MyDrive/modma_project/modma_per_subject/test\"\n",
        "\n",
        "# Pick a random file from train\n",
        "sample_file = random.choice(glob(os.path.join(train_dir, \"*.npz\")))\n",
        "print(\"üìÑ Random subject file:\", sample_file)\n",
        "\n",
        "# Load it\n",
        "data = np.load(sample_file)\n",
        "for k in data.files:\n",
        "    print(f\"{k}: shape = {data[k].shape}\")\n",
        "\n",
        "# Optional ‚Äî quick data sanity checks\n",
        "X = data[\"X\"]\n",
        "y = data[\"y\"]\n",
        "print(\"\\nX dtype:\", X.dtype, \" | min:\", X.min(), \"max:\", X.max())\n",
        "print(\"y unique values:\", np.unique(y))\n",
        "# Re-executed to show shape of a random file as requested by the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwtXR2rUP7LV",
        "outputId": "e1eaed4e-6be4-4b65-c35f-b96e07d9da5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "subject_2010002.npz unique labels: [1]\n",
            "subject_2010005.npz unique labels: [1]\n",
            "subject_2010006.npz unique labels: [1]\n",
            "subject_2010008.npz unique labels: [1]\n",
            "subject_2010010.npz unique labels: [1]\n"
          ]
        }
      ],
      "source": [
        "from glob import glob\n",
        "import numpy as np\n",
        "\n",
        "train_dir = \"/content/gdrive/MyDrive/modma_project/modma_per_subject/train\"\n",
        "\n",
        "for path in sorted(glob(os.path.join(train_dir, \"*.npz\"))[:5]):  # check first 5\n",
        "    d = np.load(path)\n",
        "    y = d[\"y\"]\n",
        "    print(os.path.basename(path), \"unique labels:\", np.unique(y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Cuzq5y0SOCM",
        "outputId": "91f2caa5-293d-4e2f-d902-e1fd3bcabf03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìÑ File: subject_2010024.npz\n",
            "  X: shape = (480, 128, 251, 1), dtype = float64\n",
            "  y: shape = (480,), dtype = int64\n",
            "  subject: shape = (), dtype = <U7\n",
            "  -> After squeeze, X shape: (480, 128, 251)\n",
            "  -> Labels unique: [1]\n",
            "\n",
            "üìÑ File: subject_2030019.npz\n",
            "  X: shape = (480, 128, 251, 1), dtype = float64\n",
            "  y: shape = (480,), dtype = int64\n",
            "  subject: shape = (), dtype = <U7\n",
            "  -> After squeeze, X shape: (480, 128, 251)\n",
            "  -> Labels unique: [0]\n",
            "\n",
            "üìÑ File: subject_2020026.npz\n",
            "  X: shape = (480, 128, 251, 1), dtype = float64\n",
            "  y: shape = (480,), dtype = int64\n",
            "  subject: shape = (), dtype = <U7\n",
            "  -> After squeeze, X shape: (480, 128, 251)\n",
            "  -> Labels unique: [0]\n",
            "\n",
            "Class balance across subjects:\n",
            "  Class 0: 25 subjects\n",
            "  Class 1: 17 subjects\n",
            "\n",
            "Class balance across subjects:\n",
            "  Class 0: 4 subjects\n",
            "  Class 1: 7 subjects\n"
          ]
        }
      ],
      "source": [
        "# Function to inspect a random subject file\n",
        "import random\n",
        "\n",
        "def inspect_random_subject(folder, n=3):\n",
        "    files = sorted(glob(os.path.join(folder, \"*.npz\")))\n",
        "    n = min(n, len(files))  # fix here\n",
        "    sample_files = random.sample(files, n)\n",
        "    for f in sample_files:\n",
        "        data = np.load(f)\n",
        "        print(f\"\\nüìÑ File: {os.path.basename(f)}\")\n",
        "        for k in data.files:\n",
        "            print(f\"  {k}: shape = {data[k].shape}, dtype = {data[k].dtype}\")\n",
        "        X = data[\"X\"].squeeze(-1)\n",
        "        y = data[\"y\"]\n",
        "        print(f\"  -> After squeeze, X shape: {X.shape}\")\n",
        "        print(f\"  -> Labels unique: {np.unique(y)}\")\n",
        "\n",
        "\n",
        "# Inspect 3 random train subjects\n",
        "inspect_random_subject(TRAIN_DIR, n=3)\n",
        "\n",
        "# Check class balance across all train subjects\n",
        "def class_balance(folder):\n",
        "    files = sorted(glob(os.path.join(folder, \"*.npz\")))\n",
        "    if len(files) == 0:\n",
        "        print(\"No files found in folder!\")\n",
        "        return\n",
        "    all_labels = []\n",
        "    for f in files:\n",
        "        d = np.load(f)\n",
        "        # pick the first label as representative of subject\n",
        "        y = d[\"y\"]\n",
        "        if len(y) == 0:\n",
        "            print(f\"Warning: {os.path.basename(f)} has no labels!\")\n",
        "            continue\n",
        "        all_labels.append(y[0])\n",
        "    if len(all_labels) == 0:\n",
        "        print(\"No labels found across subjects!\")\n",
        "        return\n",
        "    all_labels = np.array(all_labels)\n",
        "    unique, counts = np.unique(all_labels, return_counts=True)\n",
        "    print(\"\\nClass balance across subjects:\")\n",
        "    for u, c in zip(unique, counts):\n",
        "        print(f\"  Class {u}: {c} subjects\")\n",
        "\n",
        "# Example usage\n",
        "TRAIN_DIR = \"/content/gdrive/MyDrive/modma_project/modma_per_subject/train\"\n",
        "TEST_DIR  = \"/content/gdrive/MyDrive/modma_project/modma_per_subject/test\"\n",
        "class_balance(TRAIN_DIR)\n",
        "class_balance(TEST_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ7BYEBRKkaj"
      },
      "source": [
        "## 2-CSP + reimannian feature extraction + Global tangent space\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0cec528",
        "outputId": "e1ee31fb-e1de-4594-f2fa-177bff534e63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/7.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.5/7.4 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.8/7.4 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/127.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.7/127.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ Setup complete\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies (if not already)\n",
        "!pip install mne scikit-learn numpy scipy pandas pyriemann xgboost joblib --quiet\n",
        "\n",
        "# -----------------------------\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import balanced_accuracy_score, classification_report, confusion_matrix\n",
        "from mne.decoding import CSP\n",
        "from mne.filter import filter_data\n",
        "\n",
        "print(\"‚úÖ Setup complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNvqzs7xKniQ"
      },
      "outputs": [],
      "source": [
        "# ---------------- CONFIG\n",
        "TRAIN_DIR = \"/content/gdrive/MyDrive/modma_project/modma_per_subject/train\"\n",
        "TEST_DIR  = \"/content/gdrive/MyDrive/modma_project/modma_per_subject/test\"\n",
        "ART_DIR   = \"/content/gdrive/MyDrive/modma_project/Ml_approach\"\n",
        "\n",
        "os.makedirs(ART_DIR, exist_ok=True)\n",
        "\n",
        "CSP_COMPONENTS = 6\n",
        "BAND = (8, 30)\n",
        "SFREQ = 250\n",
        "EPS = 1e-6\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVagC4nQKqWJ",
        "outputId": "2794a1d1-3e91-482d-cf94-58467766cb06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train subjects: 42 | Test subjects: 11\n"
          ]
        }
      ],
      "source": [
        "def load_subject_npz(path):\n",
        "    d = np.load(path, allow_pickle=True)\n",
        "    X = d[\"X\"]\n",
        "    if X.ndim == 4: X = X.squeeze(-1)   # (trials, ch, time)\n",
        "    y = d[\"y\"]\n",
        "    subj = d[\"subject\"]\n",
        "    if getattr(subj, \"shape\", ()) == (): subj = subj.item()\n",
        "    return X, y, subj\n",
        "\n",
        "def list_npz(folder):\n",
        "    return sorted(glob(os.path.join(folder, \"*.npz\")))\n",
        "\n",
        "train_files = list_npz(TRAIN_DIR)\n",
        "test_files  = list_npz(TEST_DIR)\n",
        "print(f\"Train subjects: {len(train_files)} | Test subjects: {len(test_files)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXb6QzgbKuRV",
        "outputId": "df13a275-428b-4269-e1f4-b67ea3dfd596"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting CSP on TRAIN only...\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 19 (2.2e-16 eps * 128 dim * 6.8e+14  max singular value)\n",
            "    Estimated rank (data): 128\n",
            "    data: rank 128 computed from 128 data channels with 0 projectors\n",
            "Reducing data rank from 128 -> 128\n",
            "Estimating class=0 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=1 covariance using EMPIRICAL\n",
            "Done.\n",
            "‚úÖ CSP saved.\n"
          ]
        }
      ],
      "source": [
        "from mne.filter import filter_data\n",
        "\n",
        "def bandpass_trials(X):\n",
        "    Xf = np.zeros_like(X)\n",
        "    for i in range(X.shape[0]):\n",
        "        Xf[i] = filter_data(X[i], sfreq=SFREQ, l_freq=BAND[0], h_freq=BAND[1],\n",
        "                            method='iir', verbose=False)\n",
        "    return Xf\n",
        "\n",
        "def fit_csp_on_train(train_files, n_components=6, max_trials_per_subj=20):\n",
        "    X_fit, y_fit = [], []\n",
        "    for f in train_files:\n",
        "        X, y, _ = load_subject_npz(f)\n",
        "        n = min(max_trials_per_subj, len(y))\n",
        "        idx = np.random.choice(len(y), n, replace=False)\n",
        "        Xf = bandpass_trials(X[idx])\n",
        "        X_fit.append(Xf)\n",
        "        y_fit.append(y[idx])\n",
        "    X_fit = np.concatenate(X_fit, axis=0)\n",
        "    y_fit = np.concatenate(y_fit, axis=0)\n",
        "    csp = CSP(n_components=n_components, norm_trace=False, transform_into='csp_space')\n",
        "\n",
        "    csp.fit(X_fit, y_fit)\n",
        "    return csp\n",
        "\n",
        "print(\"Fitting CSP on TRAIN only...\")\n",
        "csp = fit_csp_on_train(train_files, n_components=CSP_COMPONENTS)\n",
        "joblib.dump(csp, os.path.join(ART_DIR, \"csp_pipeline.pkl\"))\n",
        "print(\"‚úÖ CSP saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPI5dUexKwq4",
        "outputId": "b57349ff-0c9d-468a-a4d3-cd6369f92fda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transforming CSP (train/test)...\n",
            "CSP shapes: (20160, 6, 251) (5280, 6, 251)\n"
          ]
        }
      ],
      "source": [
        "def transform_csp(files, csp):\n",
        "    X_list, y_list, g_list = [], [], []\n",
        "    for f in files:\n",
        "        X, y, subj = load_subject_npz(f)\n",
        "        Xf = bandpass_trials(X)\n",
        "        Xcsp = csp.transform(Xf)\n",
        "        X_list.append(Xcsp); y_list.append(y); g_list.append(np.repeat(subj, len(y)))\n",
        "    return np.concatenate(X_list), np.concatenate(y_list), np.concatenate(g_list)\n",
        "\n",
        "print(\"Transforming CSP (train/test)...\")\n",
        "Xtr_csp, ytr, gtr = transform_csp(train_files, csp)\n",
        "Xte_csp, yte, gte = transform_csp(test_files,  csp)\n",
        "print(\"CSP shapes:\", Xtr_csp.shape, Xte_csp.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DifT9FfKzJL",
        "outputId": "bdb818c1-fc0a-420c-d169-6fe3da81ceea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing global mean covariance...\n",
            "‚úÖ TangentSpace saved.\n"
          ]
        }
      ],
      "source": [
        "from pyriemann.estimation import Covariances\n",
        "from pyriemann.tangentspace import TangentSpace\n",
        "\n",
        "def compute_global_ts_mean_cov(train_files):\n",
        "    cov_est = Covariances(estimator='oas')\n",
        "    covs_mean = None\n",
        "    count = 0\n",
        "    for f in train_files:\n",
        "        X, y, _ = load_subject_npz(f)\n",
        "        Xf = bandpass_trials(X)\n",
        "        covs = cov_est.fit_transform(Xf)\n",
        "        subj_cov = covs.mean(axis=0)\n",
        "        covs_mean = subj_cov if covs_mean is None else (covs_mean + subj_cov)\n",
        "        count += 1\n",
        "    mean_cov = covs_mean / count\n",
        "    return mean_cov\n",
        "\n",
        "def fit_global_ts(mean_cov):\n",
        "    ts = TangentSpace()\n",
        "    ts.fit(np.expand_dims(mean_cov, axis=0))\n",
        "    return ts\n",
        "\n",
        "print(\"Computing global mean covariance...\")\n",
        "mean_cov = compute_global_ts_mean_cov(train_files)\n",
        "ts = fit_global_ts(mean_cov)\n",
        "joblib.dump(ts, os.path.join(ART_DIR, \"global_tangent_space.pkl\"))\n",
        "print(\"‚úÖ TangentSpace saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "q-j6QKnMK1X5",
        "outputId": "1e13eac6-62ff-48ef-8c4f-b29cbe8eb57f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transforming Riemann features...\n",
            "Riemann shapes: (20160, 8256) (5280, 8256)\n",
            "Stats shapes: (20160, 4, 251) (5280, 4, 251)\n",
            "CSP: (20160, 6, 251)\n",
            "Riemann: (20160, 8256)\n",
            "Stats: (20160, 4, 251)\n",
            "Aligning to minimum length: 20160\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1132454111.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mXtr_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mXtr_csp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtr_riem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtr_stats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mXte_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mXte_csp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXte_riem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXte_stats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fused:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtr_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXte_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcasting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcasting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)"
          ]
        }
      ],
      "source": [
        "def transform_riemann(files, ts):\n",
        "    cov_est = Covariances(estimator='oas')\n",
        "    X_list, y_list, g_list = [], [], []\n",
        "    for f in files:\n",
        "        X, y, subj = load_subject_npz(f)\n",
        "        Xf = bandpass_trials(X)\n",
        "        covs = cov_est.fit_transform(Xf)\n",
        "        covs += np.eye(covs.shape[1]) * EPS\n",
        "        X_ts = ts.transform(covs)\n",
        "        X_list.append(X_ts); y_list.append(y); g_list.append(np.repeat(subj, len(y)))\n",
        "    return np.concatenate(X_list), np.concatenate(y_list), np.concatenate(g_list)\n",
        "\n",
        "print(\"Transforming Riemann features...\")\n",
        "Xtr_riem, _, _ = transform_riemann(train_files, ts)\n",
        "Xte_riem, _, _ = transform_riemann(test_files,  ts)\n",
        "print(\"Riemann shapes:\", Xtr_riem.shape, Xte_riem.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnamrji9Jxxz",
        "outputId": "f612e992-25d3-4d06-8ac1-ff589bb132ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stats shapes: (20160, 24) (5280, 24)\n",
            "Flattened CSP shapes: (20160, 6) (5280, 6)\n",
            "Fused: (20160, 8286) (5280, 8286)\n"
          ]
        }
      ],
      "source": [
        "# re-define fixed stats function\n",
        "def stats_from_csp(X_csp):\n",
        "    if X_csp.ndim == 3:\n",
        "        mean = X_csp.mean(axis=2)\n",
        "        std  = X_csp.std(axis=2)\n",
        "        sk   = (((X_csp - mean[:, :, None]) / (std[:, :, None] + 1e-12))**3).mean(axis=2)\n",
        "        kt   = (((X_csp - mean[:, :, None]) / (std[:, :, None] + 1e-12))**4).mean(axis=2)\n",
        "        return np.hstack([mean, std, sk, kt])\n",
        "    else:\n",
        "        mean = X_csp.mean(axis=1, keepdims=True)\n",
        "        std  = X_csp.std(axis=1, keepdims=True)\n",
        "        sk   = (((X_csp - mean)/ (std + 1e-12))**3).mean(axis=1, keepdims=True)\n",
        "        kt   = (((X_csp - mean)/ (std + 1e-12))**4).mean(axis=1, keepdims=True)\n",
        "        return np.hstack([mean, std, sk, kt])\n",
        "\n",
        "# recompute stats + fuse only\n",
        "Xtr_stats = stats_from_csp(Xtr_csp)\n",
        "Xte_stats = stats_from_csp(Xte_csp)\n",
        "print(\"Stats shapes:\", Xtr_stats.shape, Xte_stats.shape)\n",
        "# Flatten CSP outputs if 3D (mean over time dimension)\n",
        "if Xtr_csp.ndim == 3:\n",
        "    Xtr_csp = Xtr_csp.mean(axis=2)\n",
        "    Xte_csp = Xte_csp.mean(axis=2)\n",
        "\n",
        "print(\"Flattened CSP shapes:\", Xtr_csp.shape, Xte_csp.shape)\n",
        "\n",
        "\n",
        "Xtr_final = np.hstack([Xtr_csp, Xtr_riem, Xtr_stats])\n",
        "Xte_final = np.hstack([Xte_csp, Xte_riem, Xte_stats])\n",
        "print(\"Fused:\", Xtr_final.shape, Xte_final.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMAUKqrmSxYw",
        "outputId": "1f74e359-4c24-40ed-f8ec-0fc8ca8e6cc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded fused features: (20160, 8286) (5280, 8286)\n"
          ]
        }
      ],
      "source": [
        "#THIS CELL IS JUST A PATCH , to be run only when the ram crashes\n",
        "import numpy as np\n",
        "\n",
        "# Load pre-saved fused features\n",
        "fused_path = \"/content/gdrive/MyDrive/modma_project/Ml_approach/fused_features.npz\"\n",
        "data = np.load(fused_path)\n",
        "\n",
        "Xtr_final = data[\"X_train\"]\n",
        "ytr = data[\"y_train\"]\n",
        "gtr = data[\"groups_train\"]\n",
        "\n",
        "Xte_final = data[\"X_test\"]\n",
        "yte = data[\"y_test\"]\n",
        "gte = data[\"groups_test\"]\n",
        "\n",
        "print(\"Loaded fused features:\", Xtr_final.shape, Xte_final.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "UojWI4k2K3ki",
        "outputId": "ffcf2fa5-09a1-4162-ef26-ab5ad16aafea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting StandardScaler on TRAIN...\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'joblib.dump(scaler, os.path.join(ART_DIR, \"scaler.pkl\"))\\n\\nnp.savez(os.path.join(ART_DIR, \"fused_features.npz\"),\\n         X_train=Xtr_scaled, y_train=ytr, groups_train=gtr,\\n         X_test=Xte_scaled,  y_test=yte, groups_test=gte)\\nprint(\"‚úÖ Saved: scaler.pkl, fused_features.npz\")'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "print(\"Fitting StandardScaler on TRAIN...\")\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(Xtr_final)\n",
        "Xtr_scaled = scaler.transform(Xtr_final)\n",
        "Xte_scaled = scaler.transform(Xte_final)\n",
        "\"\"\"joblib.dump(scaler, os.path.join(ART_DIR, \"scaler.pkl\"))\n",
        "\n",
        "np.savez(os.path.join(ART_DIR, \"fused_features.npz\"),\n",
        "         X_train=Xtr_scaled, y_train=ytr, groups_train=gtr,\n",
        "         X_test=Xte_scaled,  y_test=yte, groups_test=gte)\n",
        "print(\"‚úÖ Saved: scaler.pkl, fused_features.npz\")\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYNXcuhi_T7k"
      },
      "source": [
        "## 3-Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJn1heQGK54t",
        "outputId": "23f2d36b-3e79-44a0-b404-d750519b521e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM-safe XGBoost training...\n",
            "[0]\ttrain-logloss:0.63034\tval-logloss:0.63057\n",
            "[25]\ttrain-logloss:0.16703\tval-logloss:0.16921\n",
            "[50]\ttrain-logloss:0.05364\tval-logloss:0.05645\n",
            "[75]\ttrain-logloss:0.01901\tval-logloss:0.02107\n",
            "[100]\ttrain-logloss:0.00743\tval-logloss:0.00888\n",
            "[125]\ttrain-logloss:0.00328\tval-logloss:0.00430\n",
            "[150]\ttrain-logloss:0.00173\tval-logloss:0.00246\n",
            "[175]\ttrain-logloss:0.00105\tval-logloss:0.00160\n",
            "[200]\ttrain-logloss:0.00073\tval-logloss:0.00118\n",
            "[225]\ttrain-logloss:0.00056\tval-logloss:0.00095\n",
            "[250]\ttrain-logloss:0.00045\tval-logloss:0.00081\n",
            "[275]\ttrain-logloss:0.00038\tval-logloss:0.00071\n",
            "[300]\ttrain-logloss:0.00034\tval-logloss:0.00064\n",
            "[325]\ttrain-logloss:0.00030\tval-logloss:0.00059\n",
            "[350]\ttrain-logloss:0.00028\tval-logloss:0.00055\n",
            "[375]\ttrain-logloss:0.00026\tval-logloss:0.00052\n",
            "[400]\ttrain-logloss:0.00024\tval-logloss:0.00050\n",
            "[425]\ttrain-logloss:0.00023\tval-logloss:0.00048\n",
            "[450]\ttrain-logloss:0.00022\tval-logloss:0.00046\n",
            "[475]\ttrain-logloss:0.00021\tval-logloss:0.00045\n",
            "[499]\ttrain-logloss:0.00021\tval-logloss:0.00044\n",
            "‚úÖ Model saved to: /content/gdrive/MyDrive/modma_project/Ml_approach/xgb_earlystop_model.json\n",
            "Balanced Accuracy (test): 0.5638\n"
          ]
        }
      ],
      "source": [
        "#XGB\n",
        "import gc, xgboost as xgb, numpy as np\n",
        "gc.collect()\n",
        "\n",
        "print(\"RAM-safe XGBoost training...\")\n",
        "\n",
        "# light DMatrix creation (saves 30‚Äì40% memory)\n",
        "dtrain = xgb.DMatrix(Xtr_final, label=ytr)\n",
        "\n",
        "# use a stratified validation split\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=42)\n",
        "tr_idx, va_idx = next(sss.split(Xtr_final, ytr))\n",
        "dtr = xgb.DMatrix(Xtr_final[tr_idx], label=ytr[tr_idx])\n",
        "dva = xgb.DMatrix(Xtr_final[va_idx], label=ytr[va_idx])\n",
        "\n",
        "params = dict(\n",
        "    objective=\"binary:logistic\",\n",
        "    eval_metric=\"logloss\",\n",
        "    learning_rate=0.05,\n",
        "    max_depth=4,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=1.2,\n",
        "    reg_alpha=0.2,\n",
        "    seed=42,\n",
        "    tree_method=\"hist\",   # üß† less RAM\n",
        "    max_bin=256,\n",
        "    nthread=2             # throttle CPU threads\n",
        ")\n",
        "\n",
        "bst = xgb.train(\n",
        "    params,\n",
        "    dtr,\n",
        "    num_boost_round=500,\n",
        "    evals=[(dtr, \"train\"), (dva, \"val\")],\n",
        "    early_stopping_rounds=30,\n",
        "    verbose_eval=25\n",
        ")\n",
        "\n",
        "model_path = \"/content/gdrive/MyDrive/modma_project/Ml_approach/xgb_earlystop_model.json\"\n",
        "bst.save_model(model_path)\n",
        "print(\"‚úÖ Model saved to:\", model_path)\n",
        "\n",
        "# quick sanity check\n",
        "dtest = xgb.DMatrix(Xte_final, label=yte)\n",
        "preds = bst.predict(dtest)\n",
        "y_pred = (preds >= 0.5).astype(int)\n",
        "bal_acc = 0.5 * ((y_pred[yte==1]==1).mean() + (y_pred[yte==0]==0).mean())\n",
        "print(f\"Balanced Accuracy (test): {bal_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R1YKJT2ajZ6",
        "outputId": "fd4a6157-d424-4a26-e3aa-68b79bece6cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚öôÔ∏è Training SVM...\n",
            "‚úÖ SVM Balanced Accuracy (test): 0.6454\n"
          ]
        }
      ],
      "source": [
        "#SVM\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "print(\"‚öôÔ∏è Training SVM...\")\n",
        "\n",
        "svm_pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm\", SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", probability=True, random_state=42))\n",
        "])\n",
        "\n",
        "svm_pipe.fit(Xtr_final, ytr)\n",
        "preds = svm_pipe.predict(Xte_final)\n",
        "bal_acc = balanced_accuracy_score(yte, preds)\n",
        "\n",
        "print(f\"‚úÖ SVM Balanced Accuracy (test): {bal_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHLowVlgc8hp",
        "outputId": "c93af675-8bcb-4f9c-ca92-1b409aa6ec17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content/gdrive/MyDrive/modma_project/Ml_approach/svm_model.pkl']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "joblib.dump(svm_pipe, \"/content/gdrive/MyDrive/modma_project/Ml_approach/svm_model.pkl\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
